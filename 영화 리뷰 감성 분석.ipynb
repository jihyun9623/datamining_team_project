{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pymysql\n",
    "import torch\n",
    "from torchtext import data\n",
    "#import re\n",
    "import torchtext\n",
    "import os\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Okt()\n",
    "def my_tokenizer(text):\n",
    "    return_value = list(map(lambda x:x[0],okt.pos(text))) # okt.pos(text) output: [(word,kinds)], ex[(\"진짜\",\"Noun\")] I want only word(each tuple index 0)\n",
    "    if len(return_value)==0:#if length zero error ouccur\n",
    "        return_value.append(\".\") \n",
    "    return return_value\n",
    "\n",
    "def morphs_no_zero(text):\n",
    "    token = Okt().morphs(text)\n",
    "    if len(token) == 0:\n",
    "        token.append(\".\")\n",
    "    return token\n",
    "# def my_token(text): #test user func\n",
    "#   return text.split()\n",
    "dir_path = \"./trained_model\"\n",
    "SEED = 1234\n",
    "#TEXT = torch.load(os.path.join(dir_path,\"TEXT.pt\"))\n",
    "TEXT = torch.load(dir_path+\"/TEXT.pt\")\n",
    "TEXT.vocab = torch.load(dir_path+\"/TEXT_vocab.pt\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text,  text_lengths):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = morphs_no_zero(sentence)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "import os\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "# path_dir = \"/content/gdrive/My Drive/Colab Notebooks/datamining/Models/\"\n",
    "# pretrained_model_list = os.listdir(path_dir)\n",
    "# pretrained_model_list.sort()   #가장 loss가 낮은 모델을 부르기 위해 정렬 #저장양식 model_name + \"-loss_\"+str(int(loss*1000)) ex LSTM_sequential_loss_1 #모델/loss 구분 \"-\"(하이푼)\n",
    "# i = 0\n",
    "# #model_state = model.state_dict()\n",
    "# pre_train_loss = 1\n",
    "# for trained_model in pretrained_model_list:\n",
    "#   i+=1\n",
    "#   if trained_model.split(\"-\")[0] == \"sentimental_analysis_model\":\n",
    "#     #model_state = torch.load(path_dir+\"/\"+trained_model)[\"model_state\"]\n",
    "#     model.load_state_dict(torch.load(path_dir+\"/\"+trained_model)[\"model_state\"])\n",
    "#     pre_train_loss = torch.load(path_dir+\"/\"+trained_model)[\"loss\"]\n",
    "#     print(\"불러온 모델\",trained_model, \"pre_train_loss: \",pre_train_loss)   \n",
    "#     break\n",
    "#   if i == len(pretrained_model_list):\n",
    "#     print(\"저장된 같은 모델이 없습니다\")\n",
    "model.load_state_dict(torch.load(dir_path +\"/sentimental_analysis_model-loss_474.pt\",map_location=torch.device('cpu'))[\"model_state\"])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    def __init__(self, star, date, title, visit, recommend, text, movie_title):\n",
    "        self.star = star\n",
    "        self.date = date\n",
    "        #self.year, self.month, self.day = self.date.spilt('.')\n",
    "        self.title = title\n",
    "        self.visit = visit\n",
    "        self.recommend = recommend\n",
    "        self.text = text.replace(\"'\",\"\")\n",
    "        self.movie_title = movie_title.replace(\"'\",\"\")\n",
    "    def __str__(self):\n",
    "        review_info = \"별점: \" + str(self.star) + \"\\n\" +\\\n",
    "                    \"날짜: \" + self.date + \"\\n\" + \\\n",
    "                    \"제목: \" + self.title + \"\\n\" + \\\n",
    "                    \"조회: \" + str(self.visit) + \"\\n\" + \\\n",
    "                    \"추천: \" + str(self.recommend) + \"\\n\" +\\\n",
    "                    \"내용: \" + self.text + \"\\n\"\n",
    "        return review_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_in_url(url):\n",
    "    \"\"\"검색된 페이지에서 네이버 영화로 들어가는 url을 return\"\"\"\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\")\n",
    "    return soup.find(\"div\", class_=\"info_main\").find(\"h3\").find(\"a\").get(\"href\")\n",
    "def review_in_url(url):\n",
    "    \"\"\"네이버 영화에서 review로 가는 url과 page수, url에 들어가는 영화code를 리턴\"\"\"\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\")\n",
    "    div = soup.find(\"div\", class_ = \"sub_tab_area\")\n",
    "    data_list = div.select(\"ul li\")\n",
    "    \n",
    "    url_code = re.findall(\"\\d+\", url)[0] #url_code 추출\n",
    "    \n",
    "    \n",
    "    url_path = \"\"\n",
    "    root_url = \"https://movie.naver.com/movie/bi/mi\"\n",
    "    for data in data_list:\n",
    "        #print(data.find(\"a\").get(\"href\"))\n",
    "        url_path = data.find(\"a\").get(\"href\")\n",
    "        if \"review\" in url_path:\n",
    "            url_path= root_url+ url_path[1:]  #\".\"을 루트경로로 바꿔줌\n",
    "            break\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url_path).read(), \"html.parser\")\n",
    "    review_count = int(soup.find(\"div\",class_=\"top_behavior\").find(\"em\").text)\n",
    "    page_count = review_count // 10 + 1\n",
    "    return url_path, page_count, url_code\n",
    "def rank_serch_url(url):\n",
    "    \"\"\"네이버에 검색해서 영화 순위의 각 검색 url을 list로 리턴\"\"\"\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\")\n",
    "    div = soup.find(\"div\", class_=\"_content\")\n",
    "    data_list = div.select(\"ul li\")\n",
    "    naver_search_list = []\n",
    "    pre_url = \"https://search.naver.com/search.naver\"\n",
    "    \n",
    "    for data in data_list:\n",
    "        #print(data.find(\"div\", class_ = \"movie_info\").find(\"a\").get(\"href\"))\n",
    "        af_url = data.find(\"div\", class_ = \"movie_info\").find(\"a\").get(\"href\")\n",
    "        naver_search_list.append(pre_url+ af_url)\n",
    "    #print(naver_search_list)\n",
    "    return naver_search_list\n",
    "def review_num(url):\n",
    "    \"\"\"review 본문으로 접근하는 url number를 리스트로 반환\"\"\"\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\")\n",
    "    div = soup.find(\"div\", class_=\"review\")\n",
    "    data_list = div.select(\"ul li\")\n",
    "    numbers = []\n",
    "    for data in data_list:\n",
    "        #print(data.find('a').get('onclick'))\n",
    "        onclick = data.find('a').get('onclick')\n",
    "        #print(onclick)\n",
    "        if re.findall(\"\\d+\", onclick) != []:         #url에 들어가는 넘버만 획득\n",
    "            number = re.findall(\"\\d+\", onclick)[0]\n",
    "            numbers.append(number)\n",
    "    return numbers\n",
    "def one_page_crawling(url_code, numbers):\n",
    "    global cur\n",
    "    review_list = []\n",
    "    review_root_url =\"https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=\"\n",
    "    end_url = \"&code=\"+str(url_code)+\"&order=#tab\" \n",
    "    for number in numbers:\n",
    "       # try:\n",
    "            review_url = review_root_url + str(number) + end_url\n",
    "            print(review_url)\n",
    "            review_soup = BeautifulSoup(urllib.request.urlopen(review_url).read(), \"html.parser\")\n",
    "            movie_title = review_soup.find(\"div\", \"mv_info\").find(\"h3\").find(\"a\").text\n",
    "            #print(movie_title)\n",
    "            div = review_soup.find(\"div\", class_ = \"review\")\n",
    "            title = div.find(\"strong\", class_=\"h_lst_tx\").text.strip()\n",
    "            date = div.find(\"span\", class_ = \"wrt_date\").text.strip()\n",
    "            text_area = div.find(\"div\", class_ = \"user_tx_area\")\n",
    "            #print(text.select(\"span\"))\n",
    "            try:\n",
    "                star = div.find(\"div\",\"star_score\").find(\"em\").text.strip()\n",
    "            except:\n",
    "                star = 0\n",
    "            #print(star)\n",
    "            visit = div.find(\"div\",class_ = \"user_tx_info\").find(\"em\").text.strip()\n",
    "            #print(visit)\n",
    "            recommend = div.find(\"div\",class_ = \"user_tx_info\").find(\"em\",id =\"goodReviewCount\").text.strip()\n",
    "            #print(recommend)\n",
    "            #print(text_area.text)\n",
    "            text = text_area.text.strip()\n",
    "            #print(\"text:\",text)\n",
    "            #print(Review(star, date, title, visit, recommend, text))\n",
    "            review_list.append(Review(star, date, title, visit, recommend, text,movie_title))\n",
    "        #except: continue\n",
    "            \n",
    "    return review_list\n",
    "def search_url(title):\n",
    "    baseurl = 'https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query='\n",
    "    url = baseurl + urllib.parse.quote_plus(title)\n",
    "    return url\n",
    "def CRAWLING(txt):\n",
    "    root_url = \"https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query=%EC%98%81%ED%99%94%EC%88%9C%EC%9C%84\"\n",
    "    #url_list = rank_serch_url(root_url)  ###현재 영화 순위로 검색시\n",
    "\n",
    "    url_list = []\n",
    "    movie_list = []\n",
    "    #여기에서 txt받아와서 movie_list에 append\n",
    "    #movie_list = [\"겨울왕국\",\"나를 찾아줘\"]  ### 그냥 검색타이틀로 크롤링 할시\n",
    "    movie_list.append(txt)\n",
    "\n",
    "    for movie in movie_list:\n",
    "        url_list.append(search_url(movie))\n",
    "\n",
    "    url_code = 0\n",
    "    page_count = 1\n",
    "    all_review = []\n",
    "    for rank_url in url_list:\n",
    "        url = movie_in_url(rank_url)\n",
    "        url, page_count, url_code = review_in_url(url)\n",
    "\n",
    "        maximum_page = 1\n",
    "        for page in range(1,page_count+1):\n",
    "            if page > maximum_page:\n",
    "                break\n",
    "            url2 = url + \"&page=\"+str(page)\n",
    "            #print(url2)\n",
    "            #print(url, page_count, url_code)\n",
    "            #print(review_num(url))\n",
    "            numbers = review_num(url2)\n",
    "            review_list = one_page_crawling(url_code, numbers)\n",
    "            all_review.extend(review_list)\n",
    "    return all_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729629&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4728007&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4726736&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4722483&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4721574&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729153&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729076&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729105&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729071&code=175324&order=#tab\n",
      "https://movie.naver.com/movie/bi/mi/reviewread.nhn?nid=4729226&code=175324&order=#tab\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import *\n",
    "from PyQt5 import uic\n",
    "from PyQt5.QtGui import *\n",
    "from PyQt5.QtCore import *\n",
    "\n",
    "form_class = uic.loadUiType(\"movie.ui\")[0]\n",
    "\n",
    "class MyWindow(QMainWindow, form_class):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setupUi(self)\n",
    "        self.pushButton.clicked.connect(self.btn_clicked)\n",
    "        self.pushButton_2.clicked.connect(self.Next)#next\n",
    "        self.pushButton_3.clicked.connect(self.Prev)#prev\n",
    "\n",
    "    def btn_clicked(self):\n",
    "        txt = self.lineEdit.text()\n",
    "        url = CRAWLING(txt)\n",
    "        \n",
    "        all_reviews = []\n",
    "        title_context = []\n",
    "        self.title_and_context= []\n",
    "        for review in url:\n",
    "            all_reviews.append(str(review).split('\\n'))\n",
    "        for i in all_reviews:\n",
    "            temp2=[]\n",
    "            for j in i: \n",
    "                if '제목' in j:\n",
    "                    temp2.append(j)\n",
    "                elif '내용' in j:\n",
    "                    temp2.append(j)\n",
    "            title_context.append(temp2)\n",
    "        for i in title_context:\n",
    "            temp3 = []\n",
    "            for j in i: \n",
    "                if '내용' in j:\n",
    "                    temp3.append(j.split('.'))\n",
    "                else:\n",
    "                    temp3.append(j)\n",
    "            self.title_and_context.append(temp3)\n",
    "        self.index = 0\n",
    "        self.textBrowser.clear()\n",
    "        #첫번째 데이터 \n",
    "        self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "        self.textBrowser.append(self.title_and_context[0][0])\n",
    "        for l2 in self.title_and_context[0][1]:\n",
    "            pred = predict_sentiment(model, l2)\n",
    "            if pred >= 0.95:\n",
    "                #if 긍정 : settextcolor blue\n",
    "                self.textBrowser.setTextColor(QColor(Qt.blue))\n",
    "                self.textBrowser.append(l2)\n",
    "            elif pred <= 0.05:\n",
    "                #else 부정 : settextcolor red\n",
    "                self.textBrowser.setTextColor(QColor(Qt.red))\n",
    "                self.textBrowser.append(l2)\n",
    "            else:\n",
    "                self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "                self.textBrowser.append(l2)\n",
    "                \n",
    "        QMessageBox.about(self, \"message\", \"크롤링 완료\")#lineEdit에 쓰는 문자열 가져오기\n",
    "       \n",
    "                \n",
    "   \n",
    "        \n",
    "    def Next(self):\n",
    "        #QMessageBox.about(self, \"message\", \"다음\")\n",
    "        self.textBrowser.clear()\n",
    "        self.index += 1;\n",
    "        if(self.index >9):\n",
    "            self.index = 0\n",
    "        self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "        self.textBrowser.append(self.title_and_context[self.index][0])\n",
    "        for l2 in self.title_and_context[self.index][1]:\n",
    "            pred = predict_sentiment(model, l2)\n",
    "            if pred >= 0.95:\n",
    "                #if 긍정 : settextcolor blue\n",
    "                self.textBrowser.setTextColor(QColor(Qt.blue))\n",
    "                self.textBrowser.append(l2)\n",
    "            elif pred <= 0.05:\n",
    "                #else 부정 : settextcolor red\n",
    "                self.textBrowser.setTextColor(QColor(Qt.red))\n",
    "                self.textBrowser.append(l2)\n",
    "            else:\n",
    "                self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "                self.textBrowser.append(l2)\n",
    "        \n",
    "    def Prev(self):\n",
    "        #QMessageBox.about(self, \"message\", \"이전\")\n",
    "        self.textBrowser.clear()\n",
    "        self.index -= 1;\n",
    "        if(self.index <0):\n",
    "            self.index = 9\n",
    "        self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "        self.textBrowser.append(self.title_and_context[self.index][0])\n",
    "        for l2 in self.title_and_context[self.index][1]:\n",
    "            pred = predict_sentiment(model, l2)\n",
    "            if pred >= 0.95:\n",
    "                #if 긍정 : settextcolor blue\n",
    "                self.textBrowser.setTextColor(QColor(Qt.blue))\n",
    "                self.textBrowser.append(l2)\n",
    "            elif pred <= 0.05:\n",
    "                #else 부정 : settextcolor red\n",
    "                self.textBrowser.setTextColor(QColor(Qt.red))\n",
    "                self.textBrowser.append(l2)\n",
    "            else:\n",
    "                self.textBrowser.setTextColor(QColor(Qt.black))\n",
    "                self.textBrowser.append(l2)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    myWindow = MyWindow()\n",
    "    myWindow.show()\n",
    "    app.exec_()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
